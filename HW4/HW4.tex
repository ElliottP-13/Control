\documentclass[11pt]{article}
\usepackage{../EllioStyle}
\usepackage{listings}
\usepackage{mathtools}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\graphicspath{ {imgs/} }

\title{Homework 4}
\author{Elliott Pryor}
\date{18 October 2023}

\rhead{Homework 4}
\lhead{Elliott Pryor}

\begin{document}
\maketitle

\problem{1}
Show that the following two systems both have a finite escape time for any
value of $\alpha$ and any value of the initial condition specified.

\begin{enumerate}[(a)]
    \item $\dot{x} = x^\alpha, \; \alpha > 1, x(0) > 0$
    \item $\dot{x} = -x^\alpha, \; \alpha > 1 \text{ is even}, x(0) < 0$
\end{enumerate}

\soln

\begin{enumerate}[(a)]
    \item $\dot{x} = x^\alpha, \; \alpha > 1, x(0) > 0$. 
    We solve and integrate both sides:
    \begin{align*}
         \frac{1}{-\alpha + 1}x^{-\alpha + 1} &= t + c\\
         x^{-\alpha + 1} &= (-\alpha + 1)(t+c)\\
         x^{\alpha - 1} &= \frac{1}{(-\alpha + 1)t + c}\\
         \ln(x) &= \frac{1}{\ln((-\alpha + 1)t + c) (\alpha - 1)}\\
         x &= \frac{1}{((-\alpha + 1)t + c)^{\alpha - 1}}\\
    \end{align*}
    We then choose $c$ to satisfy the initial condition: $x(0) = \frac{1}{c^{\alpha - 1}} = c^{-\alpha + 1}$.
    So $c = x(0)^{\frac{1}{-\alpha + 1}}$.
    This system has a pole at $t = \frac{c}{-\alpha + 1}$, where the inner portion goes to zero.
    Thus x goes to $1/0^{\alpha - 1} = 1/0 = \infty$ at $t = x(0)^{\frac{1}{-\alpha + 1}}/(-\alpha + 1)$ (finite)

    \item We have that basically the same solution satisfies the equation,
    we simply need to change the $\frac{1}{-\alpha + 1}$ in the first step, to $\frac{-1}{-\alpha + 1}$.
    Which results in $x = \frac{1}{((\alpha - 1)t + c)^{\alpha - 1}}$, and the same value for $c$.
    $c = x(0)^{\frac{1}{-\alpha + 1}} = \sqrt[-\alpha + 1]{x(0)}$. 
    The term in front of $t$ is certainly positive ($\alpha > 1$), but our constant term is negative, since we are taking an odd root 
    thus it is unique and $x(0)$ is negative, so $c$ must be negative. 
    Then we have a division by zero at $t = \sqrt[-\alpha + 1]{x(0)}/(\alpha - 1)$.
\end{enumerate}


\problem{2}
Show that if $f_1: \reals \to \reals$ and $f_2: \reals \to \reals$ are locally Lipschitz,
then $f_1 + f_2$, $f_1 f_2$, $f_2 \circ f_1$ are all locally Lipschitz.

\soln



\begin{enumerate}
    \item We want to show $||(f_1(x) + f_2(x)) - (f_1(y) + f_2(y))|| \leq L||x - y||$.
    Norm satisfies the triangle inequality, thus \\
    $||(f_1(x) - f_1(y)) - (f_2(y) + f_2(y))|| \leq  L_1||x - y|| + L_2||x - y|| = (L_1 + L_2)||x_ - y||$ as required.
    \item We want to show $||f_1(x)f_2(x) - f_1(y)f_2(y)|| \leq L||x - y||$.
    This one is trickier
    \begin{align*}
        ||f_1(x)f_2(x) - f_1(y)f_2(y)|| &= ||f_1(x)f_2(x) - f_1(y)f_2(y) + f_1(x)f_2(y) - f_1(x)f_2(y)||\\
        &= ||f_1(x)(f_2(x) - f_2(y)) + (f_1(x) - f_1(y))f_2(y) || \\
        &\leq ||f_1(x)|| L_1| |x - y|| + ||f_2(y)|| L_2 ||x - y||\\
    \end{align*} 
    Now we need to bound $||f_1(x)||$ and $||f_2(y)||$.
    Since it is locally Lipschitz, we know that $x,y$ are in a ball of radius $r< \infty$ around some point $x_0$.
    Thus the distance $||x-y||$ is bounded, so $f$ must also be bounded.
    Thus $||f_1(x)|| \leq M_1$ and $||f_2(y)|| \leq M_2$.
    So finally, we have $||f_1(x)f_2(x) - f_1(y)f_2(y)|| \leq (M_1L_1 + M_2L_2)||x - y||$.
    \item We want to show $||f_2(f_1(x)) - f_2(f_1(y))|| \leq L||x - y||$.
    Consider $a = f_1(x)$, $b = f_1(y)$, then we have $||f_2(a) - f_2(b)|| \leq L_2||a - b||$.
    So now we have $||f_2(f_1(x)) - f_2(f_1(y))|| \leq L_2 ||f_1(x) - f_1(y)|| \leq L_2L_1||x-y||$
\end{enumerate}



\problem{3}
For each of the functions $f(x): \reals \to \reals$ given below, determine whether it is
locally Lipschitz or globally Lipschitz. If it is locally Lipschitz, identify the domain on which
it is locally Lipschitz.

\begin{enumerate}
    \item $f(x) = x^2$
    \item $f(x) = |x|$
    \item $f(x) = x + sgn(x)$
    \item $f(x) = \sin(x)$
    \item $f(x) = sat(x) = sgn(x) min(1, |x|)$
\end{enumerate}

\soln

\begin{enumerate}
    \item Is only locally Lipschitz. Its growth is faster than linear, so it is not globally Lipschitz.
    However, it is bounded on any finite interval, so it is locally Lipschitz on any finite interval.
    \item It is globally Lipschitz: $||f(x) - f(y)|| = |||x| - |y||| \leq ||x - y||$.
    \item It is locally Lipschitz: for positives: $||f(x) - f(y)|| = |x + 1 - y - 1| = |x - y|$, for negatives: $||f(x) - f(y)|| = |x - 1 - y + 1| = |x - y|$
    for alternate signs: $|x + 1 - y + 1| = |x - y + 2|$. This last case is violated near zero, so it is only locally Lipschitz, on either the positive or negative reals.
    \item It is globally Lipschitz: $||f(x) - f(y)|| = ||\sin(x) - \sin(y)|| \leq ||x - y||$.
    \item It is globally Lipschitz, since $|f(x)| \leq |x|$
\end{enumerate}



\problem{4}
use the Poincare-Bendixon's criterion to
show that the system has a periodic orbit;
$\dot{x_1} = x_2$, $\dot{x_2} = -x_1 + x_2(2 - 3x_1^2 - 2 x_2^2)$

\soln

Clearly the origin is the only equilibrium point.
We need to compute the jacobian:
$$
J = \begin{bmatrix}
    0 & 1 \\ -1 - 6x_1x_2 & 2 - 3x_1^2 - 6x_2^2
\end{bmatrix} = (\text{eval at origin}) = \begin{bmatrix}
    0 & 1 \\ -1 & 2
\end{bmatrix}
$$
Which has repeated eigenvalue 1, which is positive, so we satisfy the first condition.

Now we need to find a bounded region, where all trajectories stay within. 
Let $\mathcal{M} = \{x \in \reals^2: V(x) \leq c\}$ where $V$ is a circle $V(x) = x_1^2 + x_2^2$.
We need to show that $\dot{V} \leq 0$ on the boundary of $\mathcal{M}$.
\begin{align*}
    \dot{V} &= \frac{\partial V}{\partial x_1}f_1 + \frac{\partial V}{\partial x_2} f_2 \\
    &= 2x_1x_2 + 2x_2(-x_1 + 2x_2 - 3x_1^2x_2 - 2x_2^3)\\
    &= 2x_1x_2 - 2x_1x_2 + 4x_2^2 - 6x_1^2x_2^2 - 4x_2^4\\
    &= 4x_2^2 - 6x_1^2x_2^2 - 4x_2^4\\
    &= 2x_2^2(2 - 3x_1^2 - 2x_2^2)\\
\end{align*}
The sign of this is dictated by $2 - 3x_1^2 - 2x_2^2$, so we want $2 < 3x_1^2 + 2x_2^2$.
This is an ellipse, so we can choose $c$ to be the major axis of the ellipse, and we are done!
We choose $c > 1$ (major axis of ellipse is on y axis, with length 1) which is a circle strictly larger than the ellipse.
Thus $\dot{V} \leq 0$ on the boundary of $\mathcal{M}$, and we have a periodic orbit!

\problem{5}
Consider the following nonlinear system:
\begin{align*}
    \dot{x_1} &= x_1 (x_1^2 + x_2^2)(A - \sqrt{x_1^2 + x_2^2}) - x_2\\
    \dot{x_2} &= x_1 + x_2 (x_1^2 + x_2^2)(A - \sqrt{x_1^2 + x_2^2})
\end{align*}
Where $A$ is a positive constant. 
Show that the system has a stable limit cycle.

\soln


We convert first to polar coordinates: $x_1 = r \cos \theta$, $x_2 = r \sin \theta$, $r = \sqrt{x_1^2 + x_2^2}$, $\theta = \arctan \frac{x_2}{x_1}$.
Then we have:
\begin{align*}
    \dot{r} &= \frac{2x_1\dot{x_1} + 2x_2 \dot{x_2}}{2\sqrt{x_1^2 + x_2^2}}\\
    &= \frac{\left[2x_1^2(x_1^2 + x_2^2)(A - \sqrt{x_1^2 + x_2^2}) - 2x_1x_2\right] + \left[ 2x_1x_2 + 2x_2^2(x_1^2 + x_2^2)(A - \sqrt{x_1^2 + x_2^2}) \right]}{2\sqrt{x_1^2 + x_2^2}}\\
    &= \frac{2x_1^2r^2(A - r) - 2x_1x_2 + 2x_1x_2 + 2x_2^2r^2(A - r)}{2r}\\
    &= \frac{2r^2(A - r)(x_1^2 + x_2^2)}{2r} \\
    &= r^3(A - r)\\
\end{align*}

Next for $\dot{\theta}$:
\begin{align*}
    \dot{\theta} &= \frac{1}{1 + \left(\frac{x_2}{x_1}\right)^2} \frac{x_1\dot{x_2} - x_2\dot{x_1}}{x_1^2} \\
    &= \frac{x_1^2}{r^2} \frac{\left[ x_1^2 + x_1x_2 r^2 (A - r) \right] - \left[x_1x_2r^2(A-r) - x_2^2 \right]}{x_1^2}\\
    &= \frac{x_1^2 + x_2^2}{r^2} \\
    &= \frac{r^2(\cos^2 \theta + \sin^2 \theta)}{r^2}\\
    &= \cos^2(\theta) + \sin^2(\theta)\\
    &= 1\\
\end{align*}

So based on this, $\dot{\theta}$ is always positive, so it is always turning,
but $\dot{r} = 0$ when $r = A$, so we have a limit cycle at $r = A$.
It is stable, when $r > A$ $\dot{r} < 0$ and when $r < A$ $\dot{r} > 0$

\problem{6}

Euler equations for a rotating rigid statecraft are given by

\begin{align*}
    J_1 \dot{\omega_1} &= (J_2 - J_3)(\omega_2\omega_3) + u_1\\
    J_2 \dot{\omega_2} &= (J_3 - J_1)\omega_3\omega_1 + u_2\\
    J_3 \dot{\omega_3} &= (J_1 - J_2)\omega_1\omega_2 + u_3
\end{align*}

Where $\omega_1,\omega_2,\omega_3$ are the components of the angular velocity vector $\omega$ along the principal
axes, $u_1, u_2 u_3$ are the torque inputs applied about the principal axes, and $J_1, J_2, J_3$
are the principal moments of inertia.
\begin{enumerate}[(a)]
    \item Show that with $u_1, u_2, u_3 = 0$, the origin $\omega = 0$ is stable. Is it asymtotically stable?
    \item Suppose the torque inputs apply the feedback control $u_i = -k_i\omega_i$
    where $k_1, k_2, k_3$ are positive constants. Show that the origin $\omega = 0$ is globally
    asymptotically stable.
\end{enumerate}

\soln

\begin{enumerate}[(a)]
    \item We choose Lyaponov function: $V(x) = 1/2 w_1^2 + 1/2 w_2^2 + 1/2 w_3^2$.
    Then the derivative is:
    \begin{align*}
        \dot{V} &= \omega_1 \dot{\omega_1} + \omega_2 \dot{\omega_2} + \omega_3 \dot{\omega_3}\\
        &= \omega_1 (J_2 - J_3)(\omega_2\omega_3) + \omega_2 (J_3 - J_1)\omega_3\omega_1 + \omega_3 (J_1 - J_2)\omega_1\omega_2\\
        &= (J_2 - J_3)\omega_1\omega_2\omega_3 + (J_3 - J_1)\omega_1\omega_2\omega_3 + (J_1 - J_2)\omega_1\omega_2\omega_3\\
        &= (J_2 - J_3 + J_3 - J_1 + J_1 - J_2)\omega_1\omega_2\omega_3\\
        &= 0
    \end{align*}
    So it is stable.
    But it is not less than zero for all $\omega$, so it is not asymptotically stable.

    \item We choose Lyaponov function: $V(x) = 1/2 \omega_1^2 + 1/2 \omega_2^2 + 1/2 \omega_3^2$.
    Then the derivative is:
    \begin{align*}
        \dot{V} &= \omega_1 \dot{\omega_1} + \omega_2 \dot{\omega_2} + \omega_3 \dot{\omega_3}\\
        &= \omega_1 [(J_2 - J_3)(\omega_2\omega_3) - k_1\omega_1] + \omega_2 [(J_3 - J_1)\omega_3\omega_1 - k_2\omega_2] + \omega_3 [(J_1 - J_2)\omega_1\omega_2 - k_3\omega_3]\\
        &= (J_2 - J_3)\omega_1\omega_2\omega_3 - k_1\omega_1^2 + (J_3 - J_1)\omega_1\omega_2\omega_3 - k_2\omega_2^2 + (J_1 - J_2)\omega_1\omega_2\omega_3 - k_3\omega_3^2\\
        &= (J_2 - J_3 + J_3 - J_1 + J_1 - J_2)\omega_1\omega_2\omega_3 - k_1\omega_1^2 - k_2\omega_2^2 - k_3\omega_3^2\\
        &= -k_1\omega_1^2 - k_2\omega_2^2 - k_3\omega_3^2\\
    \end{align*}
    Which is less than zero for all $\omega \neq 0$ and $V(x) \to \infty$ as $x \to \infty$, so it is globally asymptotically stable.
    

\end{enumerate}


\problem{7}
Consider a linear system with input u and output y. Three experiments are
performed on the system using inputs $u_1(t), u_2(t), u_3(t)$ for $t \geq 0$. In each case, the initial
state $x(0)$ at time $t = 0$ is the same. The corresponding outputs are denoted by $y_1(t), y_2(t)
, y_3(t)$, respectively. Which of the following statements are correct if $x(0) \neq 0$ Which
are correct if $x(0) = 0$?

\begin{enumerate}
    \item If $u_3 = u_1 + u_2$ then $y_3 = y_1 + y_2$
    \item If $u_3 = 0.5 (u_1 + u_2)$, then $y_3 = 0.5(y_1 + y_2)$
    \item If $u_3 = u_1 - u_2$ then $y_3 = y_1 - y_2$ 
\end{enumerate}


\soln

This is about the principal of superposition which states,
$$
\begin{rcases*}
    \alpha_1x_1(t_0) + \alpha_2x_2(t_0) \\
    \alpha_1u_1(t) + \alpha_2x_2(t), \; t \geq t_0
\end{rcases*} \implies \alpha_1y_1(t) + \alpha_2y_2(t), \; t \geq t_0
$$

Case 1 is true for $x(0) = 0$, but not for $x(0) \neq 0$ because $x_3(0) = x(0)$ instead of $2x(0)$.\\
Case 2 is true for both $x(0) = 0$ and $x(0) \neq 0$.\\
Case 3 is true for $x(0) = 0$, but not for $x(0) \neq 0$ because $x_3(0) = x(0)$ instead of $0$.\\

\problem{8}

An oscillation can be generated by:
$$
\dot{x} = \begin{bmatrix}
    0 & 1\\-1 &0
\end{bmatrix}x
$$
Show that its solution is $$
x(t) = \begin{bmatrix}
    \cos t & \sin t \\ - \sin t & \cos t
\end{bmatrix}x(0)
$$

\soln

In class we learned the integrating factor method.
Which says $x(t) = e^{A(t - t_0)}x_0 + \int_{t_0}^t e^{A(t - \tau)} u(\tau) d\tau$.
In this case, we don't have any u, so we can ignore the integral.
So we just need to find $e^{At}$.

We first need eigenvalues of $A$:
$\lambda^2 + 1 = 0 \implies \lambda = \pm i$. 
Then with Cayley-Hamilton we let $f(\lambda) = e^{\lambda t}$,
and $h(\lambda) = \beta_0 + \beta_1 \lambda$.
We have the following system of equations:
$\begin{cases}
    f(i) = h(i)\\
    f(-i) = h(-i)
\end{cases} = \begin{cases}
    e^{it} = \cos t + i \sin t = \beta_0 + \beta_1 i\\
    e^{-it} = \cos t - i \sin t = \beta_0 - \beta_1 i
\end{cases}$
So $\beta_0 = \cos t$ and $\beta_1 = \sin t$

So we have $e^{At} = \cos t I + \sin t A = \begin{bmatrix}
    \cos t & \sin t \\ - \sin t & \cos t
\end{bmatrix}$.




\problem{9}
Find the unit-step response of the linear system.
\begin{align*}
    \dot{x} &= \begin{bmatrix}
        0 & 1 \\ -2 & -2
    \end{bmatrix} x + \begin{pmatrix}
        1 \\ 1
    \end{pmatrix} u\\
    y &= \begin{pmatrix}
        2 & 3
    \end{pmatrix} x
\end{align*}
\soln

So the unit step is: $u(t) = \begin{cases}
    1 & t \geq 0\\
    0 & t < 0
\end{cases}$.

We know the general form of the response is:
$y(t) = Ce^{At}x(0) + C \int_0 ^t e^{A (t - \tau)} B u(\tau) d\tau + Du(t)$.
For our case ($x(0) = 0$, $u(t) = 1$), it simplifies to 
$y(t) = C e^{At} \left( \int_0^t e^{-A\tau} d\tau  \right)B$

So first order of business is finding $e^{At}$.
We know $\lambda^2 + 2 \lambda + 2 = 0$, so $\lambda = -1 \pm i$.
Then with Cayley-Hamilton we let $f(\lambda) = e^{\lambda t}$,
and $h(\lambda) = \beta_0 + \beta_1 \lambda$.
We have the following system of equations:
$\begin{cases}
    f(-1 + i) = h(-1 + i)\\
    f(-1 - i) = h(-1 - i)
\end{cases} = \begin{cases}
    e^{-t}(\cos t + i \sin t) = \beta_0 + \beta_1 (-1 + i)\\
    e^{-t}(\cos t - i \sin t) = \beta_0 + \beta_1 (-1 - i)
\end{cases}$\\
So $\beta_0 = e^{-t} \cos t + e^{-t} \sin t$ and $\beta_1 = e^{-t} \sin t$.
Thus $e^{At} = \begin{bmatrix}
    e^{-t} \cos t + e^{-t} \sin t & e^{-t} \sin t \\ - 2e^{-t} \sin t & e^{-t} \cos t - e^{-t} \sin t
\end{bmatrix}$

We also know from the derivative property of exponential that the integral: $\int_0^t e^{-A\tau} d\tau = \Eval{e^{-At}A^{-1}(-1)}{0}{t} = A^{-1} - e^{-At}A^{-1}$.
So we have:
\begin{align*}
    y(t) &= Ce^{At} \left( A^{-1} - e^{-At}A^{-1} \right)B\\
    &= Ce^{At}A^{-1}B - Ce^{At}e^{-At}A^{-1}B\\
    &= Ce^{At}A^{-1}B - CA^{-1}B\\
\end{align*}

Then, we can find:
$A^{-1}B = \begin{bmatrix}
    -3/2\\1
\end{bmatrix}$, so $CA^{-1}B = 0$

Then we find:
$Ce^{At} = \begin{bmatrix}
    2e^{-t} \cos t + 2 e^{-t} \sin t - 6e^{-t} \sin t&
    2e^{-t} \sin t + 3e^{-t} \cos t - 3 e^{-t} \sin t
\end{bmatrix} =\\ \begin{bmatrix}
    2e^{-t} \cos t - 4 e^{-t} \sin t&
    3e^{-t} \cos t - e^{-t} \sin t
\end{bmatrix}$

Then we multiply this by $A^{-1}B$ resulting in:
$$
y(t) = -3e^{-t} \cos t + 6 e^{-t} \sin t + 3e^{-t} \cos t - e^{-t} \sin t = 5e^{-t} \sin t
$$

\problem{10}
Consider the system:
\begin{align*}
    \dot{x_1} &= -x_1\\
    \dot{x_2} &= (x_1x_2 - 1)x_2^3 + (x_1x_2 - 1 - x_1^2)x_2
\end{align*}

\begin{enumerate}[(a)]
    \item Show that the system has a unique equilibrium point
    \item Using linearization, show the equilibrium point is asymptotically stable
\end{enumerate}
\soln


\begin{enumerate}[(a)]
    \item Clearly $x_1$ must equal zero at equilibrium. So using this knowledge to solve for $x_2$, we get:
    $0 = (-1)x_2^3 -x_2$, so $x_2 = 0$ is the only real value that satisfies this equation.
    \item We compute the jacobian:
    $J_f = \begin{bmatrix}
        -1 & 0 \\ x_2^4 + x_2^2 - 2x_1x_2 & 3x_1x_2^3 + 3x_1x_2^2 + 2x_1x_2 - 1 - x_1^2
    \end{bmatrix}$,
    we evaluate at $(0,0)$:
    $J_{0,0} = \begin{bmatrix}
        -1 & 0 \\ 0 & -1
    \end{bmatrix}$, where the eigenvalues are clearly $-1, -1$ which are both negative, so it is asymptotically stable!

\end{enumerate}

\end{document}